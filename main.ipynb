{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dlib in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (20.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\ASUS\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: dlib\n",
      "Version: 20.0.0\n",
      "Summary: A toolkit for making real world machine learning and data analysis applications\n",
      "Home-page: https://github.com/davisking/dlib\n",
      "Author: Davis King\n",
      "Author-email: davis@dlib.net\n",
      "License: Boost Software License\n",
      "Location: C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\n",
      "Requires: \n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import mobilenet_v2\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Eye_model(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Eye_model, self).__init__()\n",
    "\n",
    "        # Load pretrained MobilenetV2 model\n",
    "        self.base_model = mobilenet_v2(pretrained=True)\n",
    "        out_features = self.base_model.classifier[1].out_features\n",
    "        \n",
    "        # Add custom layers\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(out_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "        \n",
    "        # Freeze base model layers\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = self.base_model(inp)\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "        return self.head(x)\n",
    "\n",
    "model = Eye_model(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_7916\\2926370483.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(r\"D:\\CSES\\Year2\\IoT\\Project\\checkpoint.pt\", map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"D:\\CSES\\Year2\\IoT\\Project\\shape_predictor_68_face_landmarks.dat\")\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(r\"D:\\CSES\\Year2\\IoT\\Project\\checkpoint.pt\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "def extract_eyes_opencv_enhanced(img, gray):\n",
    "  eye_images = []\n",
    "  faces = face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(100,100))\n",
    "  \n",
    "  for (x,y,w,h) in faces:\n",
    "      roi_gray = gray[y:y+int(h*0.6), x:x+w]  # Only upper face region\n",
    "      roi_color = img[y:y+int(h*0.6), x:x+w]\n",
    "      \n",
    "      # More sensitive parameters for eyes\n",
    "      eyes = eye_cascade.detectMultiScale(\n",
    "          roi_gray,\n",
    "          scaleFactor=1.05,\n",
    "          minNeighbors=5,\n",
    "          minSize=(30,30),\n",
    "          flags=cv2.CASCADE_SCALE_IMAGE\n",
    "      )\n",
    "      \n",
    "      # Sort by area and take largest two detections\n",
    "      eyes = sorted(eyes, key=lambda e: e[2]*e[3], reverse=True)[:2]\n",
    "      \n",
    "      for (ex,ey,ew,eh) in eyes:\n",
    "          eye_img = roi_color[ey:ey+eh, ex:ex+ew]\n",
    "          if eye_img.size > 0:\n",
    "              eye_images.append(eye_img)\n",
    "  \n",
    "  return eye_images\n",
    "\n",
    "# def extract_eyes_dlib_fast(img, gray):\n",
    "#   eye_images = []\n",
    "#   faces = detector(gray, 0)  # Faster with upsample=0\n",
    "  \n",
    "#   for face in faces:\n",
    "#       # Get only the eye landmarks (skip others)\n",
    "#       shape = predictor(gray, face)\n",
    "      \n",
    "#       # Left eye (points 36-41)\n",
    "#       lx1 = shape.part(36).x\n",
    "#       lx2 = shape.part(39).x\n",
    "#       ly1 = min(shape.part(i).y for i in range(36, 42))\n",
    "#       ly2 = max(shape.part(i).y for i in range(36, 42))\n",
    "#       left_eye = img[ly1:ly2, lx1:lx2]\n",
    "      \n",
    "#       # Right eye (points 42-47)\n",
    "#       rx1 = shape.part(42).x\n",
    "#       rx2 = shape.part(45).x\n",
    "#       ry1 = min(shape.part(i).y for i in range(42, 48))\n",
    "#       ry2 = max(shape.part(i).y for i in range(42, 48))\n",
    "#       right_eye = img[ry1:ry2, rx1:rx2]\n",
    "      \n",
    "#       if left_eye.size > 0: eye_images.append(left_eye)\n",
    "#       if right_eye.size > 0: eye_images.append(right_eye)\n",
    "  \n",
    "#   return eye_images\n",
    "\n",
    "def preprocess_eye(eye_img):\n",
    "  \"\"\"Prepare eye image for model input\"\"\"\n",
    "  transform = transforms.Compose([\n",
    "      transforms.ToPILImage(),\n",
    "      transforms.Resize((64, 64)),\n",
    "      transforms.Grayscale(num_output_channels=3),  # Convert to 3-channel if model expects RGB\n",
    "      transforms.ToTensor(),\n",
    "  ])\n",
    "  return transform(eye_img)\n",
    "\n",
    "def get_facial_landmarks(img, gray):\n",
    "    faces = detector(gray)\n",
    "    landmarks_list = []\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "        landmarks_list.append([(landmarks.part(i).x, landmarks.part(i).y) for i in range(68)])\n",
    "    return landmarks_list\n",
    "\n",
    "def display_output(img, predicted_class, landmarks_list=None):\n",
    "  height, width, _ = img.shape\n",
    "  font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "  font_scale = 2\n",
    "  thickness = 3\n",
    "  color = (0, 0, 255)  # Red\n",
    "\n",
    "  text = str(predicted_class)\n",
    "  (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, thickness)\n",
    "\n",
    "  # Bottom-right corner position (10 pixels padding)\n",
    "  x = width - text_width - 10\n",
    "  y = height - 10\n",
    "  cv2.putText(img, text, (x, y), font, font_scale, color, thickness)\n",
    "\n",
    "  # Draw facial landmarks if available\n",
    "  if landmarks_list:\n",
    "      for landmarks in landmarks_list:\n",
    "          for (x, y) in landmarks:\n",
    "              cv2.circle(img, (x, y), 2, (0, 255, 0), -1)\n",
    "\n",
    "  cv2.imshow(\"output\", img)\n",
    "  return img\n",
    "\n",
    "def save_image(img, pred_class, count, f_name):\n",
    "  if pred_class is not None:\n",
    "    cv2.imwrite('out/{}_frame-{:03d}.jpg'.format(pred_class, count), img)\n",
    "  else:\n",
    "    cv2.imwrite('{}/frame-{:03d}.jpg'.format(f_name, count), img)\n",
    "  count += 1\n",
    "  return count\n",
    "\n",
    "def define_labels():\n",
    "  labels = {\n",
    "      0 : 'aware',\n",
    "      1 : 'drowsy',\n",
    "      \n",
    "    }\n",
    "  return labels\n",
    "\n",
    "def recognize():\n",
    "    labels = define_labels()\n",
    "    cam = cv2.VideoCapture(0)\n",
    "    out_count = 1\n",
    "    thresh_count = 1\n",
    "    inp_count = 1\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "      transforms.ToPILImage(),\n",
    "      transforms.Resize((64, 64)),\n",
    "      transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      while True:\n",
    "          ret, img = cam.read()\n",
    "          if not ret:\n",
    "            break\n",
    "          img = cv2.flip(img, 1)\n",
    "          img_copy = img.copy()\n",
    "\n",
    "          # Face detection\n",
    "          gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "          faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(60, 60))\n",
    "\n",
    "          for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "          # Get facial landmarks with dlib\n",
    "          landmarks_list = get_facial_landmarks(img, gray)\n",
    "\n",
    "          # Get eye images\n",
    "          eye_images = extract_eyes_opencv_enhanced(img, gray)\n",
    "          \n",
    "          predicted_class = None\n",
    "          if len(eye_images) >= 2:  # At least both eyes detected\n",
    "            # Process each eye and average predictions\n",
    "            preds = []\n",
    "            for eye in eye_images[:2]:  # Use only first two eyes if multiple faces\n",
    "                eye_tensor = preprocess_eye(eye).unsqueeze(0)\n",
    "                pred = model(eye_tensor)\n",
    "                preds.append(pred)\n",
    "            \n",
    "            # Average predictions from both eyes\n",
    "            combined_pred = torch.mean(torch.stack(preds), dim=0)\n",
    "            predicted_class = labels[combined_pred.argmax().item()]\n",
    "          elif len(eye_images) == 1:\n",
    "            eye_tensor = preprocess_eye(eye_images[0]).unsqueeze(0)\n",
    "            pred = model(eye_tensor)\n",
    "            predicted_class = labels[pred.argmax().item()]\n",
    "\n",
    "          ret_img = display_output(img, predicted_class, landmarks_list)\n",
    "          # cv2.imshow(\"thresh\", masked_frame)\n",
    "\n",
    "          if cv2.waitKey(1) == ord('s'):\n",
    "            inp_count = save_image(img_copy, None, inp_count, 'inp')\n",
    "            out_count = save_image(ret_img, predicted_class, out_count, 'out')\n",
    "          if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "recognize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.9 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "9da8354831a1a76e7385765622e29197f528ad38b7148e1cd7d6cac41aa942b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
